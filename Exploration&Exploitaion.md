# 探索与利用 Exploration & Exploitation

## 序列决策任务中的一个基本问题
基于目前策略获取已知最优收益（exploitation）？还是尝试不同的决策（exploration）？

 由于强化学习需要自己收集数据，所以需要平衡探索和利用之间的平衡。并不是数据收集得越多越好，而是需要探索不同的决策收集到的数据带来的结果。

当前策略 ！= 最优策略

## 策略探索的一些原则

朴素方法Naive exploration：添加策略噪声 $\epsilon-greedy$（人为添加一些概率）
积极初始化Optimistic Initialization：假设所有情况都很好
基于不确定性的度量Uncertainty Measurement：尝试具有不确定收益的策略，可能带来更高收益
概率匹配Probability Matching：基于概率选择最优策略


# 多臂老虎机 multi-armed bandit
有一个拥有K根拉杆的老虎机，每个拉杆都对应一个奖励的概率分布，目标是在拉动T次拉杆后获得尽可能高的累计奖励。需要权衡：“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”

## 形式化描述：
MAB问题可以表示为一个tuple<A,R>
A为action集合，each action represents 拉动一个拉杆。若有K根拉杆，动作空间就是${a_1,..., a_K}$
R为奖励概率分布，每个动作都对应一个(usually different)奖励概率分布R(r|a)

假设每个时间步只能拉动一个拉杆，MAB目标为最大化一段时间步T内累积的奖励：$max\sum_{t=1}^T r_t, r_t~R(*|a_t)$

## 收益估计
对于每一个动作a，定义其期望收益$Q_n(a^i)=E_{r~R(.|a)}[r]$

算法流程如下：
* 对于任意a属于A，初始化计数器N(a)和期望收益估值$\widehat{Q}(a)=0$
* for t = 1 $\rightarrow$ T
*     选取某根拉杆，该动作记为$a_t$
*     得到奖励$r_t$
*     更新计数器为$N(a_t)+1$
*     更新期望收益估值
$\widehat{Q}(a_t)=\widehat{Q}(a_t)+\frac{1}{N(a_t)}[r_t-\widehat{Q}(a_t)]$
(误差项)
增量实现：空间复杂度为O(1)
（若将所有数求和再除以次数，则空间复杂度为O(n)）

## Regret函数
* 决策的期望收益：$Q(a^i)$
* 最优收益：$Q^* = maxQ(a^i)$  (最好的拉杆)

决策与最优决策的收益差：$R(a^i) = Q^* - Q(a^i)$
Total Regret 函数：$\sigma_R = E[\sum R(a^i)]$

最大化期望收益 等价于 最小化累积Regret函数

* 如果一直探索，total regret将线性递增，无法收敛
* 如果一直利用，total regret依然线性递增
是否存在次线性sublinear的阶？













